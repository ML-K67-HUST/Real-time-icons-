{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PKVdnYWPHWl"
   },
   "source": [
    "### INSTALLATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qPARitKRIeRq",
    "outputId": "893b059f-16e8-4152-d315-fe9cf3a13849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Collecting scikit-multilearn\n",
      "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
      "Successfully installed scikit-multilearn-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n",
    "!pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_BQ6RmJusMt",
    "outputId": "f8c7459e-a296-418b-d910-15366684a288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-14 04:59:51--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2024-12-14 04:59:51--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
      "\n",
      "2024-12-14 05:02:30 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B/glove.6B.50d.txt  \n",
      "  inflating: glove.6B/glove.6B.100d.txt  \n",
      "  inflating: glove.6B/glove.6B.200d.txt  \n",
      "  inflating: glove.6B/glove.6B.300d.txt  \n"
     ]
    }
   ],
   "source": [
    "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip -d glove.6B/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWAmwoVtPQct"
   },
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2swvHC-lnlT",
    "outputId": "4a4daec5-c81e-48af-b703-baa219a19448"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.metrics import hamming_loss, f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "from functools import partial\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AdamW\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lM89KGJRPVEx"
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cWu2tPrTtCUU"
   },
   "outputs": [],
   "source": [
    "docs = nltk.corpus.reuters.fileids()\n",
    "labels = [nltk.corpus.reuters.categories(doc) for doc in docs]\n",
    "texts = [nltk.corpus.reuters.raw(doc) for doc in docs]\n",
    "mlb = MultiLabelBinarizer()\n",
    "bin_labels = mlb.fit_transform(labels)\n",
    "label_names = mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aslo0xrAtluf",
    "outputId": "dc0133df-be5e-4346-ce0a-2288cb9db9ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  acq  alum  barley  bop  \\\n",
      "0  ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...    0     0       0    0   \n",
      "1  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...    0     0       0    0   \n",
      "2  JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...    0     0       0    0   \n",
      "3  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...    0     0       0    0   \n",
      "4  INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...    0     0       0    0   \n",
      "\n",
      "   carcass  castor-oil  cocoa  coconut  coconut-oil  ...  sun-oil  sunseed  \\\n",
      "0        0           0      0        0            0  ...        0        0   \n",
      "1        0           0      0        0            0  ...        0        0   \n",
      "2        0           0      0        0            0  ...        0        0   \n",
      "3        0           0      0        0            0  ...        0        0   \n",
      "4        0           0      0        0            0  ...        0        0   \n",
      "\n",
      "   tea  tin  trade  veg-oil  wheat  wpi  yen  zinc  \n",
      "0    0    0      1        0      0    0    0     0  \n",
      "1    0    0      0        0      0    0    0     0  \n",
      "2    0    0      0        0      0    0    0     0  \n",
      "3    0    1      1        0      0    0    0     0  \n",
      "4    0    0      0        1      0    0    0     0  \n",
      "\n",
      "[5 rows x 91 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'text': texts})\n",
    "for i, label in enumerate(label_names):\n",
    "    df[label] = bin_labels[:, i]\n",
    "\n",
    "print(df.head())\n",
    "labels = df.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dJCdz5uxmMGz"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\" \", sentence)\n",
    "\n",
    "def lemmatize(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        lem = lemmatizer.lemmatize(word)\n",
    "        lemSentence += lem\n",
    "        lemSentence += \" \"\n",
    "    lemSentence = lemSentence.strip()\n",
    "    return lemSentence\n",
    "\n",
    "def preprocess_text(train_df):\n",
    "    train_df['text'] = train_df['text'].str.lower()\n",
    "    train_df['text'] = train_df['text'].apply(cleanHtml)\n",
    "    train_df['text'] = train_df['text'].apply(cleanPunc)\n",
    "    train_df['text'] = train_df['text'].apply(keepAlpha)\n",
    "    train_df['text'] = train_df['text'].apply(removeStopWords)\n",
    "    train_df['text'] = train_df['text'].apply(lemmatize)\n",
    "    return train_df\n",
    "\n",
    "def create_glove_embedding_matrix(tokenizer, glove_file_path):\n",
    "    glove_embeddings = {}\n",
    "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove_embeddings[word] = vector\n",
    "\n",
    "    vocab_size = len(tokenizer.index_word) + 1\n",
    "    #File 300 dim expected\n",
    "    embedding_dim = 300\n",
    "    glove_embedding_matrix = torch.zeros(vocab_size, embedding_dim)\n",
    "\n",
    "    unknown_words = 0\n",
    "    for i in range(1, vocab_size):\n",
    "        word = tokenizer.index_word[i]\n",
    "        if word in glove_embeddings.keys():\n",
    "            glove_embedding_matrix[i] = torch.from_numpy(glove_embeddings[word]).float()\n",
    "        else:\n",
    "            unknown_words += 1\n",
    "\n",
    "    print(\"GloVe embedding matrix created!\")\n",
    "    print('Vocabulary size: {}'.format(vocab_size))\n",
    "    print('Total unknown words: {}'.format(unknown_words))\n",
    "\n",
    "    return glove_embeddings, glove_embedding_matrix\n",
    "\n",
    "def create_glove_label_embedding(labels, glove_embeddings):\n",
    "    glove_label_embedding = torch.zeros(len(labels), 300)\n",
    "\n",
    "    for index, label in enumerate(labels):\n",
    "        wrds = label.split('_')\n",
    "        for l in wrds:\n",
    "            if l in glove_embeddings.keys():\n",
    "                glove_label_embedding[index] += torch.from_numpy(glove_embeddings[l])\n",
    "        glove_label_embedding[index] /= len(wrds)\n",
    "\n",
    "    return glove_label_embedding\n",
    "\n",
    "\n",
    "#Build adjacency matrix based on Co-Occurencies label\n",
    "def create_adjacency_matrix_cooccurance(data_label):\n",
    "  cooccur_matrix = np.zeros((data_label.shape[1], data_label.shape[1]), dtype=float)\n",
    "  for y in data_label:\n",
    "      y = list(y)\n",
    "      for i in range(len(y)):\n",
    "          for j in range(len(y)):\n",
    "            #data_label\n",
    "              if y[i] == 1 and y[j] == 1:\n",
    "                  cooccur_matrix[i, j] += 1\n",
    "  row_sums = data_label.sum(axis=0)\n",
    "\n",
    "  for i in range(cooccur_matrix.shape[0]):\n",
    "    for j in range(cooccur_matrix.shape[0]):\n",
    "      if row_sums[i]!=0:\n",
    "        cooccur_matrix[i][j]=cooccur_matrix[i,j]/row_sums[i]\n",
    "      else:\n",
    "        cooccur_matrix[i][j]=cooccur_matrix[i,j]\n",
    "\n",
    "  return cooccur_matrix\n",
    "\n",
    "class dataset(Dataset):\n",
    "  def __init__(self, x, y):\n",
    "    self.x  = x\n",
    "    self.y = y\n",
    "  def __len__(self):\n",
    "    return len(self.x)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.x[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v_wY_jw_t3rI"
   },
   "outputs": [],
   "source": [
    "df = preprocess_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-0730hDzUu1",
    "outputId": "1f157bca-dca1-4bd2-c233-bc27fbceff6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10788,) (10788, 90)\n",
      "(8630,) (2158,) (8630, 90) (2158, 90)\n"
     ]
    }
   ],
   "source": [
    "X = df['text'].values\n",
    "y = df.iloc[:,1:].values\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=200)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOqCnS1uP4Kr"
   },
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wLihiGRwxOmR"
   },
   "outputs": [],
   "source": [
    "class MAGNET(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, adjacency, embeddings, heads=4, slope=0.01, dropout=0.5):\n",
    "    super(MAGNET, self).__init__()\n",
    "    self.embedding = nn.Embedding.from_pretrained(embeddings)\n",
    "    self.biLSTM = nn.LSTM(input_size,hidden_size,batch_first=True,bidirectional=True)\n",
    "    self.adjacency = nn.Parameter(adjacency)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.edge_weights = nn.Linear(hidden_size*2*2, 1, bias=False)\n",
    "    self.activation = nn.LeakyReLU(slope)\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "    self.tanh = nn.Tanh()\n",
    "    self.heads = heads\n",
    "    self.transform_dim1 = nn.Linear(input_size, hidden_size*2, bias=False)\n",
    "    self.transform_dim2 = nn.Linear(hidden_size*2, hidden_size*2, bias=False)\n",
    "    self.transform_dimensions = [self.transform_dim1, self.transform_dim2]\n",
    "\n",
    "  def forward(self, token, label_embedding):\n",
    "      #BILSTM part\n",
    "      features = self.embedding(token)\n",
    "      out, (h, _) = self.biLSTM(features)\n",
    "      embedding = torch.cat([h[-2, :, :], h[-1, :, :]], dim=1)\n",
    "      embedding = self.dropout(embedding)\n",
    "\n",
    "      #GAT PART\n",
    "      for td in self.transform_dimensions: #Two Multiheaded GAT layers\n",
    "        outputs = []\n",
    "        for head in range(self.heads):\n",
    "          label_embed = td(label_embedding)\n",
    "          n, embed_size = label_embed.shape\n",
    "\n",
    "          label_embed_combinations = label_embed.unsqueeze(1).expand(-1, n, -1)\n",
    "          label_embed_combinations = torch.cat([label_embed_combinations, label_embed.unsqueeze(0).expand(n, -1, -1)], dim=2)\n",
    "          e = self.activation(self.edge_weights(label_embed_combinations).squeeze(2))\n",
    "\n",
    "          attention_coefficients = self.tanh(torch.mul(e,self.adjacency))\n",
    "\n",
    "          new_h = torch.matmul(attention_coefficients.to(label_embed.dtype), label_embed)\n",
    "          outputs.append(new_h)\n",
    "        outputs = self.activation(torch.mean(torch.stack(outputs, dim=0),dim=0))\n",
    "\n",
    "        label_embedding = outputs\n",
    "      attention_features = self.dropout(label_embedding)\n",
    "      attention_features = attention_features.transpose(0, 1)\n",
    "      predicted_labels = torch.matmul(embedding, attention_features)\n",
    "      return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCQrqIQ1dwxJ"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, filename='MAGNET_best_model.pt'):\n",
    "    \"\"\"\n",
    "    Save the best model checkpoint\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(state, filename)\n",
    "    print(f\"Saved best model to {filename}\")\n",
    "\n",
    "def load_checkpoint(model, filename='MAGNET_best_model.pt'):\n",
    "    \"\"\"\n",
    "    Load the best model checkpoint\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"No checkpoint found\")\n",
    "        return model\n",
    "\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']} with loss {checkpoint['loss']:.5f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x5UWvoOxyWd"
   },
   "outputs": [],
   "source": [
    "def train(model, X_train, label_embedding, y_train,\n",
    "          total_epoch=250, batch_size=250, learning_rate=0.001,\n",
    "          save_path='MAGNET_best_model.pt'):\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"magnet-classification\",\n",
    "        config={\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epochs\": total_epoch,\n",
    "            \"architecture\": \"MAGNET\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    label_embedding = label_embedding.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_data = DataLoader(dataset(X_train, y_train), batch_size=batch_size)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, total_epoch + 1):\n",
    "        running_loss = 0\n",
    "        y_pred = []\n",
    "        model.train()\n",
    "\n",
    "        for index, (X, y) in enumerate(train_data):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(X.to(device), label_embedding)\n",
    "            loss = criterion(out, y.to(device).float())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "            optimizer.step()\n",
    "            y_pred.append(torch.sigmoid(out.detach()).round().cpu())\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        y_pred = torch.vstack(y_pred)\n",
    "        f1score = f1_score(y_train, y_pred, average='micro')\n",
    "        hammingloss = hamming_loss(y_train, y_pred)\n",
    "\n",
    "        # Save best model\n",
    "        if running_loss < best_loss:\n",
    "            best_loss = running_loss\n",
    "            save_checkpoint(model, optimizer, epoch, running_loss, save_path)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": running_loss,\n",
    "            \"hamming_loss\": hammingloss,\n",
    "            \"micro_f1_score\": f1score\n",
    "        })\n",
    "\n",
    "        print(f'epoch:{epoch} loss:{running_loss:.5f} hamming_loss:{hammingloss:.5f} micro_f1score:{f1score:.5f}')\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXTh-YGNP_qy"
   },
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zdQS46uBajw",
    "outputId": "c57527a1-ea1a-4bbe-b88a-717227256a35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8630, 70]) torch.Size([2158, 70]) torch.Size([8630, 90]) torch.Size([2158, 90])\n",
      "24629\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=20000,oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_text_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_text_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(pad_sequences(sequences_text_train, maxlen=70))\n",
    "X_test = torch.from_numpy(pad_sequences(sequences_text_test, maxlen=70))\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape )\n",
    "print(len(tokenizer.index_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIDBs1828WSY",
    "outputId": "b3e35a15-55e3-4f84-a5f7-4fdd30a2f5bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE : 24630\n",
      "TOTAL OF UNKNOWN WORD : 4664\n"
     ]
    }
   ],
   "source": [
    "#Load glove embedding\n",
    "glove_embeddings = {}\n",
    "with open('glove.6B/glove.6B.300d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0]\n",
    "        vector = np.array(parts[1:], dtype=np.float32)\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.index_word)+1\n",
    "glove_embedding_matrix = torch.zeros(VOCAB_SIZE, 300)\n",
    "\n",
    "unk = 0\n",
    "for i in range(1, VOCAB_SIZE):\n",
    "  word = tokenizer.index_word[i]\n",
    "  if word in glove_embeddings.keys():\n",
    "    glove_embedding_matrix[i] = torch.from_numpy(glove_embeddings[word]).float()\n",
    "  else:\n",
    "    unk +=1\n",
    "print('VOCAB_SIZE : {}'.format(VOCAB_SIZE))\n",
    "print('TOTAL OF UNKNOWN WORD : {}'.format(unk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXFcb54YzEce",
    "outputId": "1e96b99f-75bb-42f9-9be4-78b960254480"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 5.2910e-04, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         1.0582e-03],\n",
       "        [2.2222e-02, 1.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         2.2222e-02],\n",
       "        [0.0000e+00, 0.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        ...,\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [7.4074e-02, 3.7037e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         1.0000e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matrix = create_adjacency_matrix_cooccurance(y_train.numpy())\n",
    "adj_matrix = torch.tensor(adj_matrix)\n",
    "adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AOlVUO8sy5V8",
    "outputId": "8dbf3fec-4c37-4841-e69e-efdbf8f1caf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1796, -0.1051, -0.5564,  ..., -0.0633,  0.3732, -0.2873],\n",
      "        [ 0.1101,  0.4061,  0.2036,  ..., -0.1957, -0.4627,  0.6931],\n",
      "        [-0.3568, -0.1348,  0.0790,  ..., -0.0384,  0.2948,  0.1996],\n",
      "        ...,\n",
      "        [-0.1446,  0.0594, -0.1450,  ..., -0.0334,  0.1966,  0.4136],\n",
      "        [-0.5990, -0.3234, -0.2749,  ...,  0.6343,  0.5300,  0.0299],\n",
      "        [-0.4541, -0.1300, -0.5178,  ..., -1.1637, -0.2056, -0.3177]])\n"
     ]
    }
   ],
   "source": [
    "glove_label_embedding = torch.zeros(len(labels),300)\n",
    "\n",
    "for index, label in enumerate(labels):\n",
    "  wrds = label.split('-')\n",
    "  for l in wrds:\n",
    "    if l in glove_embeddings.keys():\n",
    "        glove_label_embedding[index] +=  torch.from_numpy(glove_embeddings[l])\n",
    "  glove_label_embedding[index]=glove_label_embedding[index]/len(wrds)\n",
    "\n",
    "print(glove_label_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "raUsUOqgzBIe",
    "outputId": "4cdeee6e-e6fd-41c8-baf9-07fbbecadfbf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241214_050750-wb0xk9n6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/quachtuananh1908-hanoi-university-of-science-and-technology/magnet-classification/runs/wb0xk9n6' target=\"_blank\">solar-plasma-8</a></strong> to <a href='https://wandb.ai/quachtuananh1908-hanoi-university-of-science-and-technology/magnet-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/quachtuananh1908-hanoi-university-of-science-and-technology/magnet-classification' target=\"_blank\">https://wandb.ai/quachtuananh1908-hanoi-university-of-science-and-technology/magnet-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/quachtuananh1908-hanoi-university-of-science-and-technology/magnet-classification/runs/wb0xk9n6' target=\"_blank\">https://wandb.ai/quachtuananh1908-hanoi-university-of-science-and-technology/magnet-classification/runs/wb0xk9n6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model to best_model.pt\n",
      "epoch:1 loss:9.33184 hamming_loss:0.05653 micro_f1score:0.02000\n",
      "Saved best model to best_model.pt\n",
      "epoch:2 loss:1.88525 hamming_loss:0.01386 micro_f1score:0.00037\n",
      "Saved best model to best_model.pt\n",
      "epoch:3 loss:1.70993 hamming_loss:0.01372 micro_f1score:0.01643\n",
      "Saved best model to best_model.pt\n",
      "epoch:4 loss:1.58662 hamming_loss:0.01338 micro_f1score:0.09477\n",
      "Saved best model to best_model.pt\n",
      "epoch:5 loss:1.51076 hamming_loss:0.01260 micro_f1score:0.20559\n",
      "Saved best model to best_model.pt\n",
      "epoch:6 loss:1.39362 hamming_loss:0.01174 micro_f1score:0.30165\n",
      "Saved best model to best_model.pt\n",
      "epoch:7 loss:1.23398 hamming_loss:0.00950 micro_f1score:0.52403\n",
      "Saved best model to best_model.pt\n",
      "epoch:8 loss:1.07887 hamming_loss:0.00825 micro_f1score:0.60423\n",
      "Saved best model to best_model.pt\n",
      "epoch:9 loss:0.99707 hamming_loss:0.00784 micro_f1score:0.63144\n",
      "epoch:10 loss:0.99878 hamming_loss:0.00777 micro_f1score:0.63852\n",
      "Saved best model to best_model.pt\n",
      "epoch:11 loss:0.98683 hamming_loss:0.00792 micro_f1score:0.63177\n",
      "Saved best model to best_model.pt\n",
      "epoch:12 loss:0.90126 hamming_loss:0.00742 micro_f1score:0.66551\n",
      "Saved best model to best_model.pt\n",
      "epoch:13 loss:0.84550 hamming_loss:0.00710 micro_f1score:0.68307\n",
      "Saved best model to best_model.pt\n",
      "epoch:14 loss:0.79343 hamming_loss:0.00679 micro_f1score:0.70259\n",
      "Saved best model to best_model.pt\n",
      "epoch:15 loss:0.77247 hamming_loss:0.00664 micro_f1score:0.70963\n",
      "Saved best model to best_model.pt\n",
      "epoch:16 loss:0.73877 hamming_loss:0.00643 micro_f1score:0.72285\n",
      "Saved best model to best_model.pt\n",
      "epoch:17 loss:0.69860 hamming_loss:0.00620 micro_f1score:0.73612\n",
      "Saved best model to best_model.pt\n",
      "epoch:18 loss:0.67586 hamming_loss:0.00601 micro_f1score:0.74604\n",
      "Saved best model to best_model.pt\n",
      "epoch:19 loss:0.64904 hamming_loss:0.00582 micro_f1score:0.75577\n",
      "Saved best model to best_model.pt\n",
      "epoch:20 loss:0.63754 hamming_loss:0.00580 micro_f1score:0.75695\n",
      "Saved best model to best_model.pt\n",
      "epoch:21 loss:0.60611 hamming_loss:0.00560 micro_f1score:0.76722\n",
      "Saved best model to best_model.pt\n",
      "epoch:22 loss:0.59036 hamming_loss:0.00540 micro_f1score:0.77668\n",
      "Saved best model to best_model.pt\n",
      "epoch:23 loss:0.57354 hamming_loss:0.00528 micro_f1score:0.78244\n",
      "Saved best model to best_model.pt\n",
      "epoch:24 loss:0.55543 hamming_loss:0.00528 micro_f1score:0.78293\n",
      "Saved best model to best_model.pt\n",
      "epoch:25 loss:0.54254 hamming_loss:0.00514 micro_f1score:0.79010\n",
      "Saved best model to best_model.pt\n",
      "epoch:26 loss:0.52710 hamming_loss:0.00509 micro_f1score:0.79262\n",
      "Saved best model to best_model.pt\n",
      "epoch:27 loss:0.50981 hamming_loss:0.00498 micro_f1score:0.79854\n",
      "Saved best model to best_model.pt\n",
      "epoch:28 loss:0.49214 hamming_loss:0.00479 micro_f1score:0.80643\n",
      "Saved best model to best_model.pt\n",
      "epoch:29 loss:0.47673 hamming_loss:0.00468 micro_f1score:0.81203\n",
      "Saved best model to best_model.pt\n",
      "epoch:30 loss:0.46873 hamming_loss:0.00460 micro_f1score:0.81600\n",
      "Saved best model to best_model.pt\n",
      "epoch:31 loss:0.45678 hamming_loss:0.00453 micro_f1score:0.81910\n",
      "Saved best model to best_model.pt\n",
      "epoch:32 loss:0.45653 hamming_loss:0.00458 micro_f1score:0.81804\n",
      "Saved best model to best_model.pt\n",
      "epoch:33 loss:0.43422 hamming_loss:0.00442 micro_f1score:0.82516\n",
      "Saved best model to best_model.pt\n",
      "epoch:34 loss:0.41893 hamming_loss:0.00427 micro_f1score:0.83195\n",
      "Saved best model to best_model.pt\n",
      "epoch:35 loss:0.40655 hamming_loss:0.00419 micro_f1score:0.83593\n",
      "epoch:36 loss:0.40803 hamming_loss:0.00430 micro_f1score:0.83098\n",
      "Saved best model to best_model.pt\n",
      "epoch:37 loss:0.38821 hamming_loss:0.00404 micro_f1score:0.84261\n",
      "Saved best model to best_model.pt\n",
      "epoch:38 loss:0.37043 hamming_loss:0.00382 micro_f1score:0.85197\n",
      "Saved best model to best_model.pt\n",
      "epoch:39 loss:0.36391 hamming_loss:0.00382 micro_f1score:0.85223\n",
      "Saved best model to best_model.pt\n",
      "epoch:40 loss:0.34673 hamming_loss:0.00366 micro_f1score:0.85911\n",
      "Saved best model to best_model.pt\n",
      "epoch:41 loss:0.34051 hamming_loss:0.00360 micro_f1score:0.86186\n",
      "Saved best model to best_model.pt\n",
      "epoch:42 loss:0.32740 hamming_loss:0.00349 micro_f1score:0.86664\n",
      "epoch:43 loss:0.33193 hamming_loss:0.00350 micro_f1score:0.86560\n",
      "Saved best model to best_model.pt\n",
      "epoch:44 loss:0.32347 hamming_loss:0.00347 micro_f1score:0.86782\n",
      "Saved best model to best_model.pt\n",
      "epoch:45 loss:0.32062 hamming_loss:0.00348 micro_f1score:0.86733\n",
      "Saved best model to best_model.pt\n",
      "epoch:46 loss:0.31222 hamming_loss:0.00333 micro_f1score:0.87370\n",
      "Saved best model to best_model.pt\n",
      "epoch:47 loss:0.30764 hamming_loss:0.00335 micro_f1score:0.87294\n",
      "Saved best model to best_model.pt\n",
      "epoch:48 loss:0.30368 hamming_loss:0.00328 micro_f1score:0.87546\n",
      "Saved best model to best_model.pt\n",
      "epoch:49 loss:0.29691 hamming_loss:0.00324 micro_f1score:0.87718\n",
      "Saved best model to best_model.pt\n",
      "epoch:50 loss:0.27896 hamming_loss:0.00305 micro_f1score:0.88541\n",
      "Saved best model to best_model.pt\n",
      "epoch:51 loss:0.27632 hamming_loss:0.00296 micro_f1score:0.88845\n",
      "Saved best model to best_model.pt\n",
      "epoch:52 loss:0.25757 hamming_loss:0.00280 micro_f1score:0.89545\n",
      "epoch:53 loss:0.26261 hamming_loss:0.00283 micro_f1score:0.89412\n",
      "Saved best model to best_model.pt\n",
      "epoch:54 loss:0.25118 hamming_loss:0.00273 micro_f1score:0.89798\n",
      "Saved best model to best_model.pt\n",
      "epoch:55 loss:0.24292 hamming_loss:0.00264 micro_f1score:0.90134\n",
      "epoch:56 loss:0.24695 hamming_loss:0.00272 micro_f1score:0.89854\n",
      "Saved best model to best_model.pt\n",
      "epoch:57 loss:0.24026 hamming_loss:0.00259 micro_f1score:0.90353\n",
      "Saved best model to best_model.pt\n",
      "epoch:58 loss:0.23793 hamming_loss:0.00254 micro_f1score:0.90559\n",
      "Saved best model to best_model.pt\n",
      "epoch:59 loss:0.22310 hamming_loss:0.00240 micro_f1score:0.91099\n",
      "epoch:60 loss:0.22370 hamming_loss:0.00241 micro_f1score:0.91051\n",
      "Saved best model to best_model.pt\n",
      "epoch:61 loss:0.21222 hamming_loss:0.00230 micro_f1score:0.91487\n",
      "epoch:62 loss:0.22005 hamming_loss:0.00233 micro_f1score:0.91384\n",
      "epoch:63 loss:0.22042 hamming_loss:0.00245 micro_f1score:0.90926\n",
      "Saved best model to best_model.pt\n",
      "epoch:64 loss:0.20608 hamming_loss:0.00224 micro_f1score:0.91691\n",
      "Saved best model to best_model.pt\n",
      "epoch:65 loss:0.19784 hamming_loss:0.00215 micro_f1score:0.92068\n",
      "Saved best model to best_model.pt\n",
      "epoch:66 loss:0.19645 hamming_loss:0.00210 micro_f1score:0.92232\n",
      "Saved best model to best_model.pt\n",
      "epoch:67 loss:0.19564 hamming_loss:0.00211 micro_f1score:0.92248\n",
      "epoch:68 loss:0.19778 hamming_loss:0.00215 micro_f1score:0.92076\n",
      "epoch:69 loss:0.19604 hamming_loss:0.00209 micro_f1score:0.92282\n",
      "Saved best model to best_model.pt\n",
      "epoch:70 loss:0.19336 hamming_loss:0.00209 micro_f1score:0.92314\n",
      "Saved best model to best_model.pt\n",
      "epoch:71 loss:0.18897 hamming_loss:0.00201 micro_f1score:0.92610\n",
      "epoch:72 loss:0.19054 hamming_loss:0.00199 micro_f1score:0.92678\n",
      "epoch:73 loss:0.19038 hamming_loss:0.00205 micro_f1score:0.92453\n",
      "epoch:74 loss:0.23560 hamming_loss:0.00253 micro_f1score:0.90600\n",
      "epoch:75 loss:0.20457 hamming_loss:0.00222 micro_f1score:0.91829\n",
      "epoch:76 loss:0.18958 hamming_loss:0.00203 micro_f1score:0.92514\n",
      "Saved best model to best_model.pt\n",
      "epoch:77 loss:0.17613 hamming_loss:0.00183 micro_f1score:0.93302\n",
      "Saved best model to best_model.pt\n",
      "epoch:78 loss:0.16527 hamming_loss:0.00178 micro_f1score:0.93480\n",
      "Saved best model to best_model.pt\n",
      "epoch:79 loss:0.16200 hamming_loss:0.00174 micro_f1score:0.93634\n",
      "epoch:80 loss:0.16278 hamming_loss:0.00176 micro_f1score:0.93550\n",
      "Saved best model to best_model.pt\n",
      "epoch:81 loss:0.15854 hamming_loss:0.00172 micro_f1score:0.93683\n",
      "Saved best model to best_model.pt\n",
      "epoch:82 loss:0.15556 hamming_loss:0.00168 micro_f1score:0.93863\n",
      "Saved best model to best_model.pt\n",
      "epoch:83 loss:0.15159 hamming_loss:0.00160 micro_f1score:0.94154\n",
      "epoch:84 loss:0.15279 hamming_loss:0.00166 micro_f1score:0.93923\n",
      "epoch:85 loss:0.15349 hamming_loss:0.00165 micro_f1score:0.93952\n",
      "Saved best model to best_model.pt\n",
      "epoch:86 loss:0.14895 hamming_loss:0.00160 micro_f1score:0.94167\n",
      "Saved best model to best_model.pt\n",
      "epoch:87 loss:0.14394 hamming_loss:0.00157 micro_f1score:0.94259\n",
      "Saved best model to best_model.pt\n",
      "epoch:88 loss:0.14228 hamming_loss:0.00152 micro_f1score:0.94432\n",
      "Saved best model to best_model.pt\n",
      "epoch:89 loss:0.13706 hamming_loss:0.00147 micro_f1score:0.94633\n",
      "Saved best model to best_model.pt\n",
      "epoch:90 loss:0.13293 hamming_loss:0.00145 micro_f1score:0.94696\n",
      "Saved best model to best_model.pt\n",
      "epoch:91 loss:0.12946 hamming_loss:0.00137 micro_f1score:0.95014\n",
      "epoch:92 loss:0.13379 hamming_loss:0.00144 micro_f1score:0.94766\n",
      "Saved best model to best_model.pt\n",
      "epoch:93 loss:0.12643 hamming_loss:0.00136 micro_f1score:0.95028\n",
      "Saved best model to best_model.pt\n",
      "epoch:94 loss:0.12383 hamming_loss:0.00134 micro_f1score:0.95130\n",
      "epoch:95 loss:0.12789 hamming_loss:0.00133 micro_f1score:0.95153\n",
      "epoch:96 loss:0.12700 hamming_loss:0.00139 micro_f1score:0.94938\n",
      "epoch:97 loss:0.12563 hamming_loss:0.00133 micro_f1score:0.95156\n",
      "epoch:98 loss:0.13652 hamming_loss:0.00140 micro_f1score:0.94888\n",
      "epoch:99 loss:0.12669 hamming_loss:0.00140 micro_f1score:0.94897\n",
      "epoch:100 loss:0.12785 hamming_loss:0.00136 micro_f1score:0.95041\n",
      "Saved best model to best_model.pt\n",
      "epoch:101 loss:0.12112 hamming_loss:0.00129 micro_f1score:0.95293\n",
      "epoch:102 loss:0.12710 hamming_loss:0.00139 micro_f1score:0.94927\n",
      "Saved best model to best_model.pt\n",
      "epoch:103 loss:0.11804 hamming_loss:0.00122 micro_f1score:0.95561\n",
      "epoch:104 loss:0.11833 hamming_loss:0.00128 micro_f1score:0.95348\n",
      "Saved best model to best_model.pt\n",
      "epoch:105 loss:0.10961 hamming_loss:0.00120 micro_f1score:0.95644\n",
      "Saved best model to best_model.pt\n",
      "epoch:106 loss:0.10892 hamming_loss:0.00118 micro_f1score:0.95708\n",
      "epoch:107 loss:0.11462 hamming_loss:0.00122 micro_f1score:0.95568\n",
      "epoch:108 loss:0.11072 hamming_loss:0.00117 micro_f1score:0.95745\n",
      "epoch:109 loss:0.14691 hamming_loss:0.00156 micro_f1score:0.94300\n",
      "epoch:110 loss:0.12957 hamming_loss:0.00138 micro_f1score:0.94988\n",
      "epoch:111 loss:0.11912 hamming_loss:0.00126 micro_f1score:0.95401\n",
      "epoch:112 loss:0.11233 hamming_loss:0.00122 micro_f1score:0.95563\n",
      "epoch:113 loss:0.11332 hamming_loss:0.00118 micro_f1score:0.95725\n",
      "Saved best model to best_model.pt\n",
      "epoch:114 loss:0.10471 hamming_loss:0.00112 micro_f1score:0.95942\n",
      "epoch:115 loss:0.10704 hamming_loss:0.00115 micro_f1score:0.95828\n",
      "epoch:116 loss:0.11455 hamming_loss:0.00119 micro_f1score:0.95675\n",
      "Saved best model to best_model.pt\n",
      "epoch:117 loss:0.10224 hamming_loss:0.00111 micro_f1score:0.95946\n",
      "Saved best model to best_model.pt\n",
      "epoch:118 loss:0.10016 hamming_loss:0.00109 micro_f1score:0.96043\n",
      "Saved best model to best_model.pt\n",
      "epoch:119 loss:0.09797 hamming_loss:0.00102 micro_f1score:0.96294\n",
      "epoch:120 loss:0.10078 hamming_loss:0.00111 micro_f1score:0.95961\n",
      "epoch:121 loss:0.10255 hamming_loss:0.00113 micro_f1score:0.95911\n",
      "epoch:122 loss:0.11305 hamming_loss:0.00119 micro_f1score:0.95652\n",
      "epoch:123 loss:0.10558 hamming_loss:0.00112 micro_f1score:0.95924\n",
      "epoch:124 loss:0.10291 hamming_loss:0.00110 micro_f1score:0.95985\n",
      "Saved best model to best_model.pt\n",
      "epoch:125 loss:0.09674 hamming_loss:0.00102 micro_f1score:0.96286\n",
      "Saved best model to best_model.pt\n",
      "epoch:126 loss:0.09590 hamming_loss:0.00107 micro_f1score:0.96096\n",
      "epoch:127 loss:0.09691 hamming_loss:0.00104 micro_f1score:0.96215\n",
      "Saved best model to best_model.pt\n",
      "epoch:128 loss:0.09238 hamming_loss:0.00098 micro_f1score:0.96429\n",
      "Saved best model to best_model.pt\n",
      "epoch:129 loss:0.09069 hamming_loss:0.00095 micro_f1score:0.96568\n",
      "Saved best model to best_model.pt\n",
      "epoch:130 loss:0.09041 hamming_loss:0.00099 micro_f1score:0.96398\n",
      "epoch:131 loss:0.09418 hamming_loss:0.00093 micro_f1score:0.96603\n",
      "Saved best model to best_model.pt\n",
      "epoch:132 loss:0.08754 hamming_loss:0.00093 micro_f1score:0.96624\n",
      "epoch:133 loss:0.08811 hamming_loss:0.00094 micro_f1score:0.96594\n",
      "epoch:134 loss:0.08830 hamming_loss:0.00097 micro_f1score:0.96481\n",
      "epoch:135 loss:0.09010 hamming_loss:0.00094 micro_f1score:0.96596\n",
      "epoch:136 loss:0.08844 hamming_loss:0.00098 micro_f1score:0.96443\n",
      "epoch:137 loss:0.09139 hamming_loss:0.00098 micro_f1score:0.96438\n",
      "epoch:138 loss:0.10782 hamming_loss:0.00108 micro_f1score:0.96067\n",
      "epoch:139 loss:0.10994 hamming_loss:0.00109 micro_f1score:0.96027\n",
      "epoch:140 loss:0.09648 hamming_loss:0.00101 micro_f1score:0.96342\n",
      "epoch:141 loss:0.08909 hamming_loss:0.00091 micro_f1score:0.96696\n",
      "epoch:142 loss:0.10069 hamming_loss:0.00099 micro_f1score:0.96397\n",
      "epoch:143 loss:0.09874 hamming_loss:0.00105 micro_f1score:0.96176\n",
      "epoch:144 loss:0.10163 hamming_loss:0.00108 micro_f1score:0.96076\n",
      "Saved best model to best_model.pt\n",
      "epoch:145 loss:0.08602 hamming_loss:0.00090 micro_f1score:0.96720\n",
      "epoch:146 loss:0.08958 hamming_loss:0.00092 micro_f1score:0.96664\n",
      "epoch:147 loss:0.09154 hamming_loss:0.00095 micro_f1score:0.96554\n",
      "Saved best model to best_model.pt\n",
      "epoch:148 loss:0.08021 hamming_loss:0.00084 micro_f1score:0.96954\n",
      "Saved best model to best_model.pt\n",
      "epoch:149 loss:0.07710 hamming_loss:0.00081 micro_f1score:0.97060\n",
      "epoch:150 loss:0.07829 hamming_loss:0.00082 micro_f1score:0.97016\n",
      "epoch:151 loss:0.07972 hamming_loss:0.00084 micro_f1score:0.96961\n",
      "epoch:152 loss:0.07985 hamming_loss:0.00085 micro_f1score:0.96913\n",
      "epoch:153 loss:0.08133 hamming_loss:0.00086 micro_f1score:0.96860\n",
      "Saved best model to best_model.pt\n",
      "epoch:154 loss:0.07435 hamming_loss:0.00079 micro_f1score:0.97150\n",
      "epoch:155 loss:0.07811 hamming_loss:0.00079 micro_f1score:0.97130\n",
      "epoch:156 loss:0.07682 hamming_loss:0.00081 micro_f1score:0.97063\n",
      "Saved best model to best_model.pt\n",
      "epoch:157 loss:0.07384 hamming_loss:0.00079 micro_f1score:0.97126\n",
      "Saved best model to best_model.pt\n",
      "epoch:158 loss:0.07141 hamming_loss:0.00072 micro_f1score:0.97377\n",
      "epoch:159 loss:0.07267 hamming_loss:0.00076 micro_f1score:0.97247\n",
      "epoch:160 loss:0.07675 hamming_loss:0.00085 micro_f1score:0.96915\n",
      "epoch:161 loss:0.07401 hamming_loss:0.00080 micro_f1score:0.97102\n",
      "epoch:162 loss:0.07393 hamming_loss:0.00078 micro_f1score:0.97167\n",
      "epoch:163 loss:0.07157 hamming_loss:0.00074 micro_f1score:0.97311\n",
      "Saved best model to best_model.pt\n",
      "epoch:164 loss:0.07051 hamming_loss:0.00071 micro_f1score:0.97413\n",
      "epoch:165 loss:0.10869 hamming_loss:0.00102 micro_f1score:0.96308\n",
      "epoch:166 loss:0.09204 hamming_loss:0.00097 micro_f1score:0.96477\n",
      "epoch:167 loss:0.08510 hamming_loss:0.00089 micro_f1score:0.96785\n",
      "epoch:168 loss:0.07712 hamming_loss:0.00079 micro_f1score:0.97144\n",
      "epoch:169 loss:0.07457 hamming_loss:0.00082 micro_f1score:0.97022\n",
      "epoch:170 loss:0.07357 hamming_loss:0.00075 micro_f1score:0.97266\n",
      "epoch:171 loss:0.07657 hamming_loss:0.00082 micro_f1score:0.97011\n",
      "epoch:172 loss:0.07256 hamming_loss:0.00074 micro_f1score:0.97310\n",
      "epoch:173 loss:0.07379 hamming_loss:0.00079 micro_f1score:0.97116\n",
      "epoch:174 loss:0.07326 hamming_loss:0.00077 micro_f1score:0.97202\n",
      "Saved best model to best_model.pt\n",
      "epoch:175 loss:0.06411 hamming_loss:0.00068 micro_f1score:0.97527\n",
      "epoch:176 loss:0.07296 hamming_loss:0.00077 micro_f1score:0.97188\n",
      "epoch:177 loss:0.09558 hamming_loss:0.00092 micro_f1score:0.96675\n",
      "epoch:178 loss:0.08697 hamming_loss:0.00084 micro_f1score:0.96968\n",
      "epoch:179 loss:0.07150 hamming_loss:0.00071 micro_f1score:0.97404\n",
      "epoch:180 loss:0.07726 hamming_loss:0.00076 micro_f1score:0.97224\n",
      "epoch:181 loss:0.07095 hamming_loss:0.00075 micro_f1score:0.97267\n",
      "epoch:182 loss:0.07065 hamming_loss:0.00072 micro_f1score:0.97388\n",
      "epoch:183 loss:0.07155 hamming_loss:0.00074 micro_f1score:0.97311\n",
      "epoch:184 loss:0.07061 hamming_loss:0.00076 micro_f1score:0.97240\n",
      "epoch:185 loss:0.06426 hamming_loss:0.00066 micro_f1score:0.97596\n",
      "Saved best model to best_model.pt\n",
      "epoch:186 loss:0.06093 hamming_loss:0.00064 micro_f1score:0.97659\n",
      "epoch:187 loss:0.06267 hamming_loss:0.00067 micro_f1score:0.97566\n",
      "Saved best model to best_model.pt\n",
      "epoch:188 loss:0.05908 hamming_loss:0.00060 micro_f1score:0.97812\n",
      "epoch:189 loss:0.06430 hamming_loss:0.00066 micro_f1score:0.97609\n",
      "epoch:190 loss:0.06407 hamming_loss:0.00066 micro_f1score:0.97594\n",
      "epoch:191 loss:0.06374 hamming_loss:0.00063 micro_f1score:0.97710\n",
      "epoch:192 loss:0.06673 hamming_loss:0.00072 micro_f1score:0.97397\n",
      "epoch:193 loss:0.06253 hamming_loss:0.00066 micro_f1score:0.97599\n",
      "epoch:194 loss:0.06244 hamming_loss:0.00068 micro_f1score:0.97547\n",
      "epoch:195 loss:0.06304 hamming_loss:0.00069 micro_f1score:0.97501\n",
      "epoch:196 loss:0.06262 hamming_loss:0.00068 micro_f1score:0.97521\n",
      "epoch:197 loss:0.06091 hamming_loss:0.00062 micro_f1score:0.97735\n",
      "epoch:198 loss:0.06302 hamming_loss:0.00064 micro_f1score:0.97687\n",
      "epoch:199 loss:0.05986 hamming_loss:0.00063 micro_f1score:0.97705\n",
      "epoch:200 loss:0.06183 hamming_loss:0.00065 micro_f1score:0.97633\n",
      "epoch:201 loss:0.06221 hamming_loss:0.00063 micro_f1score:0.97711\n",
      "epoch:202 loss:0.05965 hamming_loss:0.00065 micro_f1score:0.97652\n",
      "Saved best model to best_model.pt\n",
      "epoch:203 loss:0.05731 hamming_loss:0.00062 micro_f1score:0.97752\n",
      "Saved best model to best_model.pt\n",
      "epoch:204 loss:0.05703 hamming_loss:0.00058 micro_f1score:0.97912\n",
      "epoch:205 loss:0.06336 hamming_loss:0.00067 micro_f1score:0.97585\n",
      "epoch:206 loss:0.06396 hamming_loss:0.00063 micro_f1score:0.97717\n",
      "epoch:207 loss:0.06685 hamming_loss:0.00070 micro_f1score:0.97478\n",
      "epoch:208 loss:0.06620 hamming_loss:0.00068 micro_f1score:0.97549\n",
      "epoch:209 loss:0.06735 hamming_loss:0.00068 micro_f1score:0.97514\n",
      "epoch:210 loss:0.05986 hamming_loss:0.00063 micro_f1score:0.97721\n",
      "epoch:211 loss:0.06409 hamming_loss:0.00065 micro_f1score:0.97661\n",
      "epoch:212 loss:0.06288 hamming_loss:0.00064 micro_f1score:0.97671\n",
      "epoch:213 loss:0.06282 hamming_loss:0.00066 micro_f1score:0.97600\n",
      "Saved best model to best_model.pt\n",
      "epoch:214 loss:0.05554 hamming_loss:0.00058 micro_f1score:0.97885\n",
      "epoch:215 loss:0.05959 hamming_loss:0.00060 micro_f1score:0.97818\n",
      "Saved best model to best_model.pt\n",
      "epoch:216 loss:0.05365 hamming_loss:0.00056 micro_f1score:0.97971\n",
      "Saved best model to best_model.pt\n",
      "epoch:217 loss:0.05365 hamming_loss:0.00054 micro_f1score:0.98053\n",
      "epoch:218 loss:0.05969 hamming_loss:0.00060 micro_f1score:0.97821\n",
      "epoch:219 loss:0.06213 hamming_loss:0.00062 micro_f1score:0.97734\n",
      "epoch:220 loss:0.05925 hamming_loss:0.00060 micro_f1score:0.97837\n",
      "epoch:221 loss:0.05748 hamming_loss:0.00060 micro_f1score:0.97830\n",
      "epoch:222 loss:0.05687 hamming_loss:0.00061 micro_f1score:0.97781\n",
      "epoch:223 loss:0.05548 hamming_loss:0.00060 micro_f1score:0.97837\n",
      "epoch:224 loss:0.05682 hamming_loss:0.00060 micro_f1score:0.97826\n",
      "epoch:225 loss:0.05453 hamming_loss:0.00059 micro_f1score:0.97863\n",
      "epoch:226 loss:0.05758 hamming_loss:0.00059 micro_f1score:0.97844\n",
      "epoch:227 loss:0.05992 hamming_loss:0.00061 micro_f1score:0.97780\n",
      "epoch:228 loss:0.05918 hamming_loss:0.00061 micro_f1score:0.97789\n",
      "epoch:229 loss:0.05546 hamming_loss:0.00056 micro_f1score:0.97963\n",
      "epoch:230 loss:0.12435 hamming_loss:0.00127 micro_f1score:0.95404\n",
      "epoch:231 loss:0.08771 hamming_loss:0.00093 micro_f1score:0.96625\n",
      "epoch:232 loss:0.07510 hamming_loss:0.00078 micro_f1score:0.97177\n",
      "epoch:233 loss:0.07589 hamming_loss:0.00078 micro_f1score:0.97167\n",
      "epoch:234 loss:0.07741 hamming_loss:0.00076 micro_f1score:0.97229\n",
      "epoch:235 loss:0.07749 hamming_loss:0.00075 micro_f1score:0.97263\n",
      "epoch:236 loss:0.06242 hamming_loss:0.00063 micro_f1score:0.97726\n",
      "epoch:237 loss:0.06320 hamming_loss:0.00067 micro_f1score:0.97576\n",
      "epoch:238 loss:0.06188 hamming_loss:0.00064 micro_f1score:0.97686\n",
      "epoch:239 loss:0.05845 hamming_loss:0.00060 micro_f1score:0.97832\n",
      "epoch:240 loss:0.05995 hamming_loss:0.00062 micro_f1score:0.97753\n",
      "epoch:241 loss:0.06537 hamming_loss:0.00065 micro_f1score:0.97650\n",
      "epoch:242 loss:0.05957 hamming_loss:0.00062 micro_f1score:0.97745\n",
      "epoch:243 loss:0.05433 hamming_loss:0.00056 micro_f1score:0.97967\n",
      "epoch:244 loss:0.05537 hamming_loss:0.00054 micro_f1score:0.98037\n",
      "Saved best model to best_model.pt\n",
      "epoch:245 loss:0.05194 hamming_loss:0.00055 micro_f1score:0.98013\n",
      "epoch:246 loss:0.05312 hamming_loss:0.00057 micro_f1score:0.97917\n",
      "epoch:247 loss:0.05818 hamming_loss:0.00061 micro_f1score:0.97785\n",
      "epoch:248 loss:0.05683 hamming_loss:0.00058 micro_f1score:0.97879\n",
      "epoch:249 loss:0.05232 hamming_loss:0.00055 micro_f1score:0.98016\n",
      "Saved best model to best_model.pt\n",
      "epoch:250 loss:0.04792 hamming_loss:0.00050 micro_f1score:0.98188\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>hamming_loss</td><td>█▅▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>micro_f1_score</td><td>▁▂▅▆▆▇▇▇▇▇██████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>250</td></tr><tr><td>hamming_loss</td><td>0.0005</td></tr><tr><td>loss</td><td>0.04792</td></tr><tr><td>micro_f1_score</td><td>0.98188</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-plasma-8</strong> at: <a href='https://wandb.ai/quachtuananh1908-hanoi-university-of-science-and-technology/magnet-classification/runs/wb0xk9n6' target=\"_blank\">https://wandb.ai/quachtuananh1908-hanoi-university-of-science-and-technology/magnet-classification/runs/wb0xk9n6</a><br/> View project at: <a href='https://wandb.ai/quachtuananh1908-hanoi-university-of-science-and-technology/magnet-classification' target=\"_blank\">https://wandb.ai/quachtuananh1908-hanoi-university-of-science-and-technology/magnet-classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241214_050750-wb0xk9n6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = MAGNET(300, 250, adj_matrix, glove_embedding_matrix,heads=8)\n",
    "train(model, X_train, glove_label_embedding,y_train,total_epoch=250, save_path='MAGNET_best_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4GC5CNeQEf4"
   },
   "source": [
    "### EVALUATING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5UwdjQBlLm5W",
    "outputId": "d1bba17c-4276-4a73-cd45-ef3061aa1eb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-3f058c8e2b79>:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from epoch 250 with loss 0.04792\n",
      "Test Results:\n",
      "Loss: 0.74860\n",
      "Micro F1: 0.84440\n",
      "Hamming Loss: 0.00397\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, X_test, label_embedding, y_test, batch_size=250):\n",
    "    model.eval()\n",
    "    test_data = DataLoader(dataset(X_test, y_test), batch_size=batch_size)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    y_pred = []\n",
    "    test_loss = 0\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_data:\n",
    "            out = model(X.to(device), label_embedding.to(device))\n",
    "            loss = criterion(out, y.to(device).float())\n",
    "            test_loss += loss.item()\n",
    "            y_pred.append(torch.sigmoid(out).round().cpu())\n",
    "\n",
    "    y_pred = torch.vstack(y_pred)\n",
    "\n",
    "    # Calculate metrics\n",
    "    f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    hammingloss = hamming_loss(y_test, y_pred)\n",
    "\n",
    "    print(f\"Test Results:\")\n",
    "    print(f\"Loss: {test_loss:.5f}\")\n",
    "    print(f\"Micro F1: {f1_micro:.5f}\")\n",
    "    print(f\"Hamming Loss: {hammingloss:.5f}\")\n",
    "\n",
    "    return y_pred, test_loss\n",
    "\n",
    "# Usage:\n",
    "# First load your saved model if necessary\n",
    "# model = load_checkpoint(model, 'MAGNET_best_model.pt')\n",
    "predictions, test_loss = evaluate(model, X_test, glove_label_embedding, y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
